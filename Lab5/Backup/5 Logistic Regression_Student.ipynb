{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) of ``LogisticRegression`` function from ``sklearn.linear_model`` for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn = pd.read_csv('churn.csv', sep=' ')     # modify your data path if needed\n",
    "\n",
    "display(churn.head(), churn.dtypes)           # some variables are object (string or mixed) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Convert Target Variable as Numbers (Optional)\n",
    "\n",
    "Although **scikit-learn** accepts non-numeric target variable.  The classifier automatically assign integers to each class (i.e., `'LEAVE'` = `0`, `'STAY'` = `1`) before model training.  If so, \n",
    "\n",
    "- The positive class is `STAY`, then the log_odds and probability returned by the linear equation represent the log_odds and probability of ``STAY``.\n",
    "- In the array of class probabilities, ``LEAVE`` is in the 1st column and `STAY` is in the 2nd column, as we saw in week 3. \n",
    "- The parameter values will be the reverse (e.g., negative instead of positive) from what we have here. \n",
    "\n",
    "Here we convert the target as numbers mannually,  as we expect ``'LEAVE'`` as positive class (``1``) and ``'STAY'`` as  negative class (``0``). \n",
    "\n",
    "- Here we use `pandas.Series.replace` function (check [documentation](https://pandas.pydata.org/docs/reference/api/pandas.Series.replace.html)) to replace target values as numbers. After conversion, the target's data type changed from `object` (mixed data type) to  `int64` automatically.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn['LEAVE'] = churn['LEAVE'].replace({'LEAVE':1, 'STAY':0})    # Please don't opt-int to future behavior\n",
    "\n",
    "churn.head()    # now LEAVE is considered as positive (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes** \n",
    "\n",
    "If we use conditional selection for this, the target's data type is still `object`. The ambiguous data type will confuse the logistic regression algorithm as it expects `integer` or `category` data type for a numeric target.  To address this, we need to manually change the target's data type as either `int64` or  `category`. See the codes below.\n",
    "  \n",
    "```python\n",
    "churn.loc[churn['LEAVE']=='LEAVE','LEAVE'] = 1        # conditional selection\n",
    "churn.loc[churn['LEAVE']=='STAY','LEAVE'] = 0\n",
    "churn = churn.astype({'LEAVE': 'int64'})              # or churn = churn.astype({'LEAVE': 'category'}) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Convert Non-Numeric Features as Numbers (Required)\n",
    "\n",
    "As **scikit-learn** can only take numeric predictors, we need to convert all string features as numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert COLLEGE as numbers \n",
    "\n",
    "churn['COLLEGE'] = churn['COLLEGE'].replace({'zero':0, 'one':1})  # alternatively, use conditional selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use `pandas.Categorical` function to convert the string data as an ordered categorical variables and obtain the numeric codes.\n",
    "\n",
    "- Check [documentation](https://pandas.pydata.org/docs/reference/api/pandas.Categorical.html) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique values for the three features\n",
    "\n",
    "for col in ['REPORTED_SATISFACTION','REPORTED_USAGE_LEVEL','CONSIDERING_CHANGE_OF_PLAN']:\n",
    "    print(col, churn[col].unique())      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert REPORTED_SATISFACTION as numeric (very_unsat = 0, unsat=1... very_sat =4)\n",
    "\n",
    "# step 1 - convert the variable as a categorical variable (a pandas 1D array)\n",
    "cat1 = pd.Categorical(values = churn['REPORTED_SATISFACTION'], \n",
    "                      categories = [\"very_unsat\", \"unsat\", \"avg\", \"sat\",\"very_sat\"],  # specify the order\n",
    "                      ordered = True)                                                # treat categories as ordered\n",
    "\n",
    "# step 2 - obtain the numeric codes and update original feature\n",
    "churn['REPORTED_SATISFACTION'] = cat1.codes           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert REPORTED_USAGE_LEVEL as numeric\n",
    "\n",
    "# step 1\n",
    "cat2 = pd.Categorical(values = churn['REPORTED_USAGE_LEVEL'], \n",
    "                      categories = [\"very_little\", \"little\", \"avg\", \"high\",\"very_high\"],   \n",
    "                      ordered = True)\n",
    "\n",
    "# step 2\n",
    "churn['REPORTED_USAGE_LEVEL'] = cat2.codes     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert CONSIDERING_CHANGE_OF_PLAN as numeric\n",
    "\n",
    "# step 1\n",
    "cat3 = pd.Categorical(values = churn['CONSIDERING_CHANGE_OF_PLAN'], \n",
    "                      categories=[\"never_thought\", \"no\", \"perhaps\", \"considering\",\"actively_looking_into_it\"], \n",
    "                      ordered= True)\n",
    "\n",
    "# step 2\n",
    "churn['CONSIDERING_CHANGE_OF_PLAN'] = cat3.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.head(5)     # check first 5 rows  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the data type again, make sure the data type of a numeric target is either `int64` or `category`.  \n",
    "- If we use the original string target, then it is fine to be `object` data type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "churn.dtypes    # check data type: target is now int64 (discrete)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Split into Train and Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = churn.drop(columns = 'LEAVE')\n",
    "\n",
    "y = churn['LEAVE']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  \n",
    "\n",
    "display(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Scale Data\n",
    "\n",
    "Fit the scaler with the training set, then apply the same scaler to transform the trainand the test set later.\n",
    "**Do NOT** fit the scaler with the test data: referencing the test data can lead to data leakage. \n",
    "\n",
    "- Remember to use scaled data for model training and prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()                          \n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)   # combine train and transform together\n",
    "\n",
    "X_test_scaled  = scaler.transform(X_test)        # apply the scaler on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After scaling, the transformed data is a numpy array without col names. We add the names back by converting them as a dataframes with column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns)  \n",
    "\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns = X_train.columns)\n",
    "\n",
    "display(X_train_scaled.head(), X_test_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train m1 with only two Features  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.1 Model Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sub = X_train_scaled[['COLLEGE', 'INCOME']]    # 2D features\n",
    "\n",
    "m1 = LogisticRegression().fit(X_train_sub, y_train)   \n",
    "\n",
    "display(m1.intercept_, m1.coef_, m1.feature_names_in_)  # Note that intercept in 1D, coeffs in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2  Predict and Evaluate on Train Set\n",
    "\n",
    "**Predict class labels** \n",
    "\n",
    "- When making predictions, the default cut-off point for class determination is 50%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred1 = m1.predict(X_train_sub)  \n",
    "\n",
    "train_pred1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimate class probabilities**  \n",
    "\n",
    "- Note the order should be  0 (`STAY`), 1 (`LEAVE`). The probability of 1 (i.e., `LEAVE`) is in the 2nd column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prob1 = m1.predict_proba(X_train_sub)\n",
    "\n",
    "train_prob1     # probability of 0 (STAY), 1 (LEAVE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check model accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(train_pred1, y_train)     # m1.score(X_train_sub, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3  The log_odds and Probabilities of LEAVE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the formulas you may need to use.\n",
    "\n",
    "\n",
    "- Estimate the ``log_odds`` of LEAVE (i.e., ``f(x)``) with the linear function. \n",
    "\n",
    "$$\n",
    "f(x) = w_0 + w_1 \\cdot \\text{COLLEGE} + w_2 \\cdot \\text{INCOME}\n",
    "$$\n",
    "\n",
    "\n",
    "- Estimate  the ``probability`` of LEAVE with the logistic function. \n",
    "\n",
    "$$\n",
    "P(Y=1 | x) = \\frac{1}{1 + e^{-f(x)}} \\quad \\text{or} \\quad P(Y=1 |x) = \\frac{e^{f(x)}}{1 + e^{f(x)}}\n",
    "$$\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>***Exercise 1: Your Codes Here***</font>  \n",
    "\n",
    "Please complete the following two tasks:\n",
    "\n",
    "- Step 1: Calculate the log-odds of `LEAVE(1)` for the 1st instance in the training set.\n",
    "\n",
    "- Step 2: Calculate the probability of `LEAVE(1)` for the 1st instance as well. You may want to use  ``numpy.exp`` function to perform natural exponential function (with base e) to a value.\n",
    "\n",
    "Note (1) the intercept is in a 1D array and coefs are in 2D array (1*2), and (2) you may use  **.loc** or **.iloc** method to select the first customer's **COLLEGE** and **INCOME** value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate the log_odds and probability of LEAVE(1) for all instances**\n",
    "\n",
    "The ``log_odds`` values for all instances are returned by the ``decision_function`` method.  \n",
    "\n",
    "-  ``log_odds`` is proportional to instances' perpendicular distance to the hyperplane, and also called as ``confidence scores``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_odds2 = m1.decision_function(X_train_sub)   \n",
    "\n",
    "log_odds2             # the first value is the log-odds for the 1st instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the ``log_odds`` for all intances, we can calcualte the ``leaving probability`` for all customers very effectively. \n",
    "\n",
    "- The estimated leave probabilities should be the same as those returned by ``m1.predict_proba(X_train_sub)``, which is in the 2nd column of  **train_prob1**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob2 = 1 /(1 + np.exp(-log_odds2))    # element-wise computation applies   \n",
    "\n",
    "prob2                                  # same as train_prob1[:,1]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Predict and Evaluate on Test Set\n",
    "\n",
    "When making class predictions,  the default threshhold is 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sub = X_test_scaled[['COLLEGE', 'INCOME']]     # Two features for m1\n",
    "\n",
    "test_pred1 = m1.predict(X_test_sub)\n",
    "\n",
    "accuracy_score(test_pred1, y_test)                   # same as m1.score(X_test_sub, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  3.  Train m2 with all features \n",
    "\n",
    "<font color=red>***Exercise 2: Your Codes Here***</font>  \n",
    "\n",
    "Please complete the following two steps: \n",
    "- Step 1: please train **m2** with all 11 features (i.e.,  **X_train_scaled**). Check the parameter values.\n",
    "- Step 2: evaluate **m2**'s performance on both train and test set. With more features used, is the model performance better?  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "71fc9e39bb98fd0c2d992fdc358df3ee69a4d5a7dc2838964c70a2fe2b8a7efb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
